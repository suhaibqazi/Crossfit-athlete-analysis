{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_first_part = \"https://games.crossfit.com/athlete/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suhaib.qazi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\suhaib.qazi\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped page due to some missing data: 1\n",
      "page:2\n",
      "first athlete: 2 - athlete: Sara du Plessis\n",
      "page:3\n",
      "page:4\n",
      "page:5\n",
      "page:6\n",
      "skipped page due to some missing data: 6\n",
      "page:7\n",
      "page:8\n",
      "page:9\n",
      "skipped page due to some missing data: 9\n"
     ]
    }
   ],
   "source": [
    "df_year = pd.DataFrame(data=['2017','2016','2015','2014','2013','2012','2011','2010','2009','2008'],columns=['column_A'])\n",
    "\n",
    "try:\n",
    "    del(df_main)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "for i in range(1,200000):\n",
    "    print('page:'+str(i))\n",
    "    try:\n",
    "        del(df_pb)\n",
    "        del(df_R)\n",
    "        del(df3)\n",
    "        del(open_results)\n",
    "        del(df_measure)\n",
    "    except:\n",
    "        pass    \n",
    "\n",
    "#     parsed_page = html.fromstring(webtext)\n",
    "    try:\n",
    "\n",
    "        url_1 = urlopen(url_first_part+str(i+100000))\n",
    "        soup = BeautifulSoup(url_1)\n",
    "\n",
    "        name = ' '.join(soup.title.string.split()[1:-3])\n",
    "       \n",
    "        a = 'completed page '+str(i)+' - athlete: '+name\n",
    "\n",
    "\n",
    "        #region = soup.find('div',class_='text').text.strip().split('\\n')\n",
    "        \n",
    "#get athlete profile        \n",
    "\n",
    "        X = []\n",
    "        Y = []\n",
    "\n",
    "        for el in soup.findAll('li',class_='item sm-stacked'):\n",
    "\n",
    "            cells = el.find_all('div',class_='item-label')\n",
    "            val1=el.find_all('div', {'class': re.compile('^text lg')})\n",
    "\n",
    "            X.append(cells[0].find(text=True))\n",
    "            Y.append(val1[0].find(text=True))\n",
    "\n",
    "        X = [item.strip() for item in X if str(item)]\n",
    "        Y = [item.strip() for item in Y if str(item)]\n",
    "\n",
    "        df_measure=pd.DataFrame(X,columns=['column_A'])\n",
    "        df_measure['column_B']=Y\n",
    "\n",
    "\n",
    "#get athlete workout PBs        \n",
    "        \n",
    "        A=[]\n",
    "        B=[]\n",
    "\n",
    "        for el in soup.findAll('table', class_='stats'):\n",
    "            for row in el.find_all(\"tr\"):\n",
    "\n",
    "                cells = row.find_all('td')\n",
    "                workout=row.findAll('th')\n",
    "\n",
    "                A.append(cells[0].find(text=True))\n",
    "                B.append(workout[0].find(text=True))\n",
    "\n",
    "\n",
    "        df_pb=pd.DataFrame(B,columns=['column_A'])\n",
    "        df_pb['column_B']=A\n",
    "\n",
    "#get additional athlete\n",
    "         \n",
    "        E = []\n",
    "        F = []\n",
    "\n",
    "\n",
    "\n",
    "        for el in soup.find_all('li',class_='item sm-inline'):\n",
    "\n",
    "            cells = el.find_all('div',class_='item-label')\n",
    "            val1=el.find_all('div', class_='text')\n",
    "\n",
    "\n",
    "            E.append(cells[0].text)\n",
    "            F.append(val1[0].text)\n",
    "\n",
    "        E = [item.strip() for item in E if str(item)]\n",
    "        F = [item.strip() for item in F if str(item)]\n",
    "\n",
    "        df_infor=pd.DataFrame(E,columns=['column_A'])\n",
    "        df_infor['column_B']=F\n",
    "\n",
    "        \n",
    "        \n",
    "#get Open Results        \n",
    "\n",
    "        raw_open_results = list(filter(None,soup.find(\"tbody\").text.strip().split('\\n')))\n",
    "        \n",
    "        R=[]\n",
    "\n",
    "        for y in range(0,int(len(raw_open_results)/4)):\n",
    "            L=raw_open_results[y*4:(y*4)+2]\n",
    "            R.append(L)\n",
    "            \n",
    "        df_R = pd.DataFrame(R,columns=['column_A','column_B'])\n",
    "        open_results = pd.merge(df_year,df_R,how='left',on='column_A')\n",
    "        \n",
    "        \n",
    "        df3 = df_measure.append(df_pb).append(df_infor).append(open_results)\n",
    "        \n",
    "        df3.set_index('column_A',inplace=True)\n",
    "        df3.transpose()\n",
    "        df_merged = df3.T\n",
    "        #print(df_merged)\n",
    "        \n",
    "        df_merged['Iter_Number'] = int(i)\n",
    "        df_merged.set_index('Iter_Number', inplace=True)\n",
    "        \n",
    "        \n",
    "        #df_merged.insert(loc=0, column='Name', value=name)\n",
    "        df_merged['Name']=name\n",
    "        #df_merged.insert(loc=1, column='Region', value=region)\n",
    "        #print(a)\n",
    "        #print(df_merged)\n",
    "        try: \n",
    "            try:\n",
    "                df_main = df_main.append(df_merged)\n",
    "            except ValueError:\n",
    "                print('skipped page due to misaligned columns: '+str(i)+' - athlete: '+name)\n",
    "                #print(df_merged)\n",
    "                pass\n",
    "\n",
    "        except NameError:    \n",
    "            print('first athlete: '+str(i)+' - athlete: '+name)\n",
    "            #print(df_merged)\n",
    "            df_main = df_merged.copy()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    except:\n",
    "        #print(df_merged)\n",
    "        print('skipped page due to some missing data: '+str(i))\n",
    "        pass\n",
    "        #del(df_merged)\n",
    "        #print(df_merged)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_main.to_csv(r'C:\\Users\\suhaib.qazi\\Desktop\\cf_1test.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
